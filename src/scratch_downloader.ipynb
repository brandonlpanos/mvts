{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import drms\n",
    "import h5py\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from astropy.io import fits\n",
    "from pandas import Series as s\n",
    "from datetime import datetime as dt_obj\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_aq():\n",
    "    def __init__(self, series, keywords, seg, Harp_NUM = None, time = None, filter = None):\n",
    "        super().__init__()\n",
    "        self.time = time\n",
    "        self.series = series\n",
    "        self.keywords = keywords\n",
    "        self.seg = seg\n",
    "        self.filter = filter\n",
    "\n",
    "        y = 0\n",
    "        n = 0\n",
    "\n",
    "        self.Dat_len = 0\n",
    "\n",
    "        self.c = drms.Client()\n",
    "\n",
    "        if time == None:\n",
    "            self.time = f\"0-3000\"\n",
    "\n",
    "        if type(Harp_NUM) == int or Harp_NUM == None:\n",
    "            if filter != None and Harp_NUM != None:\n",
    "                name = f\"{series}[{Harp_NUM}][{self.time}][{filter}]\"\n",
    "                self.Harp_NUM = [Harp_NUM]\n",
    "\n",
    "            elif filter != None and Harp_NUM == None:\n",
    "                name = f\"{series}[][{self.time}][{filter}]\"\n",
    "                self.Harp_NUM = None\n",
    "\n",
    "            elif filter == None and Harp_NUM != None:\n",
    "                name = f\"{series}[{Harp_NUM}][{self.time}][]\"\n",
    "                self.Harp_NUM = [Harp_NUM]\n",
    "\n",
    "            else:\n",
    "                assert \"More Information is need, e.g. HARP-Numbers, or filter!\"\n",
    "\n",
    "            self.named_seg = [f\"{k.strip()}\" for k in seg.split(\",\")]\n",
    "\n",
    "            self.keys, _ = self.c.query(f\"{name}\", key=f\"{keywords}\", seg=f\"{seg}\")\n",
    "\n",
    "            if hasattr(self.keys, \"T_OBS\"):\n",
    "                print(f\"Download data in range {min(self.keys.T_OBS)}-{max(self.keys.T_OBS)}\")\n",
    "                self.Dat_len += self.keys.T_OBS.size\n",
    "\n",
    "            else:\n",
    "                print(\"Attribute 'T_OBS' does not exist in 'keys'\")\n",
    "                \n",
    "        \n",
    "        else:\n",
    "            self.keys = []\n",
    "            No_T_Obs = []\n",
    "            HARPS = np.arange(Harp_NUM[0],Harp_NUM[-1]+1,1)\n",
    "            self.Harp_NUM = []\n",
    "\n",
    "            self.named_seg = [f\"{k.strip()}\" for k in seg.split(\",\")]\n",
    "            \n",
    "            for N in tqdm(HARPS):\n",
    "                if filter == None:\n",
    "                    name = f\"{series}[{N}][{self.time}][]\"\n",
    "                else:\n",
    "                    name = f\"{series}[{N}][{self.time}][{filter}]\"\n",
    "\n",
    "                Keys, _ = self.c.query(f\"{name}\", key=f\"{keywords}\", seg=f\"{seg}\")\n",
    "\n",
    "                if hasattr(Keys, 'T_OBS'):\n",
    "                    # print(f\"Download HARP_NUM {N} in range {min(Keys.T_OBS)}-{max(Keys.T_OBS)}\")\n",
    "                    self.Dat_len += Keys.T_OBS.size\n",
    "                    self.Harp_NUM.append(N)\n",
    "                    self.keys.append(Keys)\n",
    "                    y +=1\n",
    "                else:\n",
    "                    # print(f\"Attribute .T_OBS does not exist in 'keys' for HARP_NUM {N}\")\n",
    "                    n +=1\n",
    "                    No_T_Obs.append(N)\n",
    "            print(f\"{y} HARPS to download\")\n",
    "            print(f\"{n} HARPS to dismiss\")\n",
    "\n",
    "            with open(\"No_T_OBS.txt\", 'w') as file:\n",
    "                for item in No_T_Obs:\n",
    "                    file.write(str(item) + '\\n')\n",
    "        \n",
    "    @staticmethod\n",
    "    def parse_tai_string(tstr,datetime=True):\n",
    "        \"\"\"function to convert T_OBS into a datetime object\"\"\"\n",
    "        year   = int(tstr[:4])\n",
    "        month  = int(tstr[5:7])\n",
    "        day    = int(tstr[8:10])\n",
    "        hour   = int(tstr[11:13])\n",
    "        minute = int(tstr[14:16])\n",
    "        if datetime: return dt_obj(year,month,day,hour,minute)\n",
    "        else: return year,month,day,hour,minute\n",
    "\n",
    "    def overview(self, N = None):\n",
    "        t0 = (16*60+13.9)/2622 # t0 in seconds for 2622 fit files\n",
    "        m0 = 15/2622 # memory in GB for 2622 fit files\n",
    "        print(f\"total number of fit files {self.Dat_len}\")\n",
    "        print(f\"expected memory consumption: {round(self.Dat_len*m0,1)} GB\")\n",
    "        print(f\"expected time to download: {round(self.Dat_len*t0/3600,1)} h\")\n",
    "         \n",
    "    def download(self, filename=\"Brandon\"):\n",
    "        data = h5py.File(f\"/sml/witmerj/Sunspot_Data/{filename}\", \"w\") # create data file\n",
    "\n",
    "        if self.Harp_NUM is None:\n",
    "            self.Harp_NUM = s.unique(self.keys.HARPNUM)\n",
    "\n",
    "        HARPS = [f\"H_{i}\" for i in self.Harp_NUM]\n",
    "\n",
    "        progress_bar = tqdm(total=len(self.Harp_NUM), desc=\"Overall Progress\", position=0)\n",
    "        with requests.Session() as session:\n",
    "            for h, N in zip(HARPS, self.Harp_NUM):\n",
    "                HARP_group = data.create_group(f\"{h}\") # create group for saving everything with certain harp number\n",
    "\n",
    "                if self.filter is not None:\n",
    "                    name = f\"{self.series}[{N}][{self.time}][{self.filter}]\"\n",
    "                else:\n",
    "                    name = f\"{self.series}[{N}][{self.time}]\"\n",
    "\n",
    "                keys_H, segments_H = self.c.query(name, key=self.keywords, seg=self.seg) # Download Data stepwise for each HARP\n",
    "\n",
    "                t_obs = HARP_group.create_dataset(\"t_obs\", shape=(keys_H.T_OBS.size,), dtype=\"S23\", chunks=True, maxshape=(None,))\n",
    "                datasets = {dataset_name: HARP_group.create_dataset(dataset_name, shape=(keys_H.T_OBS.size,) + fits.open('http://jsoc.stanford.edu' + getattr(segments_H, dataset_name)[0])[-1].data.shape, dtype=\"f\", chunks=True) for dataset_name in self.named_seg}\n",
    "\n",
    "                def download_dataset(dataset_name, i):\n",
    "                    url = 'http://jsoc.stanford.edu' + getattr(segments_H, dataset_name)[i]\n",
    "                    dataset = datasets[dataset_name]\n",
    "                    dataset[i] = torch.from_numpy(fits.open(url)[-1].data)\n",
    "\n",
    "                def download_t_obs(i):\n",
    "                    t_obs[i] = self.parse_tai_string(keys_H.T_OBS[i], datetime=True).isoformat()\n",
    "\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    for i in range(keys_H.T_OBS.size):\n",
    "                        executor.submit(download_t_obs, i)\n",
    "                        for dataset_name in self.named_seg:\n",
    "                            executor.submit(download_dataset, dataset_name, i)\n",
    "\n",
    "                progress_bar.update(1)\n",
    "\n",
    "        progress_bar.close()\n",
    "        data.close()\n",
    "    \n",
    "    def get_data(self, filename = \"Data.h5\"):\n",
    "        h5_file = h5py.File(f\"/sml/witmerj/Sunspot_Data/{filename}\", 'r') # open H5PY file\n",
    "\n",
    "        group_names = list(h5_file.keys())\n",
    "\n",
    "        datasets = {}\n",
    "\n",
    "        for group_name in group_names:\n",
    "            group = h5_file[group_name] # Access the group\n",
    "            datasets[group_name] = {}\n",
    "\n",
    "            datasets[group_name][\"t_obs\"] = group[\"t_obs\"][:]\n",
    "\n",
    "            for dataset_name in self.named_seg:\n",
    "                dataset = group[dataset_name][:]\n",
    "                datasets[group_name][dataset_name] = torch.from_numpy(dataset)\n",
    "        return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = data_aq('hmi.sharp_cea_720s', \"T_OBS, HARPNUM, NOAA_AR, LONDTMIN, LONDTMAX\", \"continuum, magnetogram, bitmap, Dopplergram, Br, conf_disambig\", Harp_NUM = 377, filter = None )\n",
    "dat.overview()\n",
    "dat.download(\"Brandon_8923.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envmvts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
